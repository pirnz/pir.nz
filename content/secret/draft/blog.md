# Copyright, free culture and the Internet

Humans have been creating tools since the dawn of times. And since the
beginning they wanted something back. Invention and creation is a
process that requires experience, knowledge, materials, tools and time.

Pottery, iron work, agriculture... all were discovered by some groups
and spred to the rest.

Maybe back then the time it took for an invention to spread around was
enough to provide the firs inventors a good advantage.

Sometimes they didn\'t want to share it with others. It\'s the case of
iron work or paper or silk. That can bring huge benefits, but once it
becomes public, you better have a plan B.

Sharing inventions has helped humans progress since forever, but we
wouldn\'t have so many of them if there wasn\'t anything in for the
inventors. So, how have we managed it throughout history?

[Privilege and Property - 1. From gunpowder to print: the common origins
of co...](https://books.openedition.org/obp/1062?lang=en) [Compiler
Press Introduction](http://www.compilerpress.ca/intro.htm) [Primary
Sources on Copyright - Home](http://www.copyrighthistory.org/cam/)

Ancient people considered inventions a gift from the gods: [History and
Evolution of Intellectual Property \| ABOU
NAJA](https://abounaja.com/blogs/history-of-intellectual-property). The
first record of patent like protection we have is Greece. In 500 BCE, a
city gave inventors all profits from their invention for a year. After
that there are records in England, Venice and Florentine in the XIV and
XV centuries.[History of patent law -
Wikipedia](https://en.wikipedia.org/wiki/History_of_patent_law)

It becomes clear that a higher power is needed in order to grant the
monopoly and protect it from aggressions. No government --> No
protection.

When creations started to be .... COPYRIGHT VS PATENTS

Copyright started after the pressed prints.

As√≠, a fecha de 2021 est√°n en el dominio p√∫blico las obras de los
autores fallecidos en 1940 o antes.

<https://en.wikipedia.org/wiki/Copyright_law_of_the_United_States#Duration_of_copyright>

However, works published before March 8, 1926 (other than sound
recordings) have made their way into the public domain.
![](https://upload.wikimedia.org/wikipedia/commons/7/77/Tom_Bell%27s_graph_showing_extension_of_U.S._copyright_term_over_time.svg)

Los NFT o algo as√≠ intentan implementar una escasez de productos para
elevar precios.

## There are No Good Arguments for Intellectual Property

<http://c4sif.org/2010/12/there-are-no-good-arguments-for-intellectual-property/>

## The patent troll that won a \$308M jury trial against Apple

<https://www.letterspatent.org/p/meet-the-patent-troll-that-won-a>

# Monote√≠smo vs otros, este y oeste, individuo y colectivo, dominaci√≥n y coexistencia, ajedrez y go

## Xi Jinping challenges US global leadership, warns against decoupling

<https://timesofindia.indiatimes.com/world/china/xi-jinping-challenges-us-global-leadership-warns-against-decoupling/articleshow/82158759.cms>

# A perspective on what we see

Life on Earth

In this post I want to answer a very practical question: how does a
human eye perceive electromagnetic radiation of different frequencies.

Electromagnetic radiation != light

<https://en.m.wikipedia.org/wiki/Light> Light or visible light is
electromagnetic radiation within the portion of the electromagnetic
spectrum that can be perceived by the human eye.\[1\] Visible light is
usually defined as having wavelengths in the range of 400--700 nm

<https://www.bbc.com/future/article/20150727-what-are-the-limits-of-human-vision>

<https://www.physicsclassroom.com/class/light/Lesson-2/Visible-Light-and-the-Eye-s-Response>
Sensitivity and colour

[vision - How can some animals see ultraviolet or infrared light? -
Biology
St...](https://biology.stackexchange.com/questions/40249/how-can-some-animals-see-ultraviolet-or-infrared-light)

[Evolution of the eye -
Wikipedia](https://en.wikipedia.org/wiki/Evolution_of_the_eye#Stages_of_eye_evolution)

Desde 0hz (mucho trabajo) Nombrar los efectos de las bandas m√°s
conocidas con fondo negro, y de repente la pantalla es roja amarilla etc
y se vuelve a hacer negro y empieza a quemar y radiar

Video o P√°gina web O peque√±o slider

# Companies take over states [[blog]{.smallcaps}]{.tag tag-name="blog"} {#companies-take-over-states}

## Chapter 2

At first they didn\'t pay the taxes, but soon they were closing a deal
for a pilot program in Per√∫.

A company started to replace state as service providers. This sparked an
anti-corporations wave in south and East Europe, Russia, China and
India. The \"pro-humans\", as they called themselves, had been growing
in the last centuries.

The world was again in two blocks, completely opposite.

Both sides will commit crimes before taken down the walls.

The Second Cold War was fought mainly on the Internet, with continents
loosing all connection for years (that includes all control on energy
supply and control of the population). That brought countries to old
forgotten analogic ways. There were also some violent clashes here and
there.

¬øEstamos ante una segunda revoluci√≥n industrial? Es la revoluci√≥n de los
servicios. Una √©poca de creaci√≥n de bloques.

# African kinship

African kinship with uncle\'s and aunts. Everyone is family, either
uncle\'s or aunt\'s.

# Como hacer que el capitalismo sea eficiente energ√©ticamente

Hacer que todos seamos responsables de nuestros residuos.

# Publicado

## [DONE]{.done .DONE} *as a Service* for everything [[personal]{.smallcaps}]{.tag tag-name="personal"} {#as-a-service-for-everything export_file_name="aaS_for_everything" export_date="<2021-04-11 Sun>" export_hugo_custom_front_matter=":summary We have seen a boom on subscription based services. How this will move forward?" id="20210808T190451.484042"}

Today you have subscriptions for everything.

You need something more than once but not enough to buy one? Choose one
of our monthly fees and enjoy it at your convenience (conditions apply).
You can find from cars, house rentals and healthcare, to laundry,
underwear and media content (of all sorts). I have questions about this.

This can become a nightmare for someone that has to manage 20-30
subscriptions. What happens when that persons moves to another city? Or
changes bank? Will be able to tell Amazon and Netflix to share my
personal information? Or could I pay for a subscription aggregator that
manages them for me?

Some internet providers offer some streaming services. They must get a
better rate from Prime or Disney+, which then they resell. Could I get
special access to all streaming platforms to offer customers access to
all of them as a single service? Something like \"pay 40‚Ç¨/month and
enjoy Netflix, Prime, HBO and Disney+, with X restrictions\". I don\'t
think they like the idea, but I see this very likely to happen.

How long until a company offers to switch your state taxes for a
subscription? They could negotiate with the state a better rate, and
then offer a private healthcare, benefits and pensions. The state could
save some money. Wait, private corporations replacing states? That\'s
another idea.

On the other side of the story, anyone can set up a Patreon or an
OnlyFans and ask for what they consider fair for their content. With
more and more questions on how mainstream social networks manage data,
ownership and revenue, this seems logic. Will this pay-to-follow trend
grow more and more? Instagram already has a *close friends* feature,
where you can publish content only for them. This could be the first
step towards letting friends in for free while charging strangers. Is
this a way of making social connections *valuable*?

Here are some ideas:

-   Social relationships as a Service
-   Parenting as a Service
-   Employees as a Service

Things are getting weird, let\'s see how it goes.

## [DONE]{.done .DONE} \*Hello world [[meta]{.smallcaps}]{.tag tag-name="meta"} {#hello-world export_file_name="hello_world" export_date="<2020-10-25 dom>" export_hugo_custom_front_matter=":summary My first entry in the blog"}

Hi üëã,

Being the first post, I\'ll explain what this site is going to be: a
blog.

I\'ll post here things I learn. Mostly things I encounter in my daily
life keeping a home server and using Ubuntu. I plan to write other stuff
as well, but that will come later. It is going to be more a personal
exercise than a hobby, so sorry if things are not super clear.

I chose Hugo because is damn fast and easy to setup. And as well because
is integrated pretty nicely with org-mode, an *extension* of Emacs that
has gotten my attention in the last month. I\'ll write about it when I
feel ready üò≥.

For now, I\'m going to finish here and do *C-c C-e H-H* to post this
entry in my Hugo website. Thanks for passing by.

## [DONE]{.done .DONE} Install npm packages globally in Ubuntu [[npm]{.smallcaps}]{.tag tag-name="npm"}¬†[[linux]{.smallcaps}]{.tag tag-name="linux"} {#install-npm-packages-globally-in-ubuntu export_file_name="npm_ubuntu" export_date="<2020-10-26 lun>" export_hugo_custom_front_matter=":summary Three solutions when you get permission errors when installing global packages"}

Trying to install global packages using npm in Ubuntu can end with
permission errors. This is because the default path to install those
packages is `/usr/local/lib` and `/usr/local/bin`. A quick
`ls -l /usr/local/` shows that everything is owned by root.

To solve these problems, there are three solutions (although npm says
[there are
two](https://docs.npmjs.com/resolving-eacces-permissions-errors-when-installing-packages-globally)):

### Use Node Version Manager (NVM)

This tool allows you to have different Node.js versions installed at the
same time and to switch between them when you need to. They are stored
in `~/.nvm/`, and any global packages are stored in there. This may seem
an overkill if you are just starting with Node.js, but it really isn\'t.

With time, different projects may need different Node versions, or
package versions, and this is the best way to organize everything. You
are killing two birds in one shot. Find the instructions to install nvm
[here](https://github.com/nvm-sh/nvm#installing-and-updating). And by
the way, you don\'t need to remove anything you already have before
installing it.

npm also recommends this option:

> This is the best way to avoid permissions issues. To reinstall npm
> with a node version manager, follow the steps in \"[Downloading and
> installing Node.js and
> npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\".
> You do not need to remove your current version of npm or Node.js
> before installing a node version manager.

*[Resolving EACCES permissions errors when installing packages
globally](https://docs.npmjs.com/resolving-eacces-permissions-errors-when-installing-packages-globally#reinstall-npm-with-a-node-version-manager)*

### Change npm\'s default global directory

If there\'s problems with the default directory, just change it. Save
your global packages in `~/.npm-global`. To do this:

-   Create a directory for global installations:

`mkdir ~/.npm-global`

-   Configure npm to use the new directory path:

`npm config set prefix '~/.npm-global'`

-   Open or create a `~/.profile` file and add this line:

`export PATH=~/.npm-global/bin:$PATH`

-   [@4]On the command line, update your system variables:

`source ~/.profile`

What this does is tell npm to install global packages to a folder you
own (it\'s in your home folder), and also add to the `PATH` variable the
new route, so that it will find the executables when you type them in
the terminal.

### Change folder permissions

Last option, not mentioned by npm, is to change the permissions of
`/usr/local/lib` and `/usr/local/bin`. Easy and fast, just run:

``` shell
sudo chown -R <user>:<user> /usr/local/lib
sudo chown -R <user>:<user> /usr/local/bin
```

This way, you can run npm commands without sudo and install global
packages. Downside, you are messing with Linux default permissions. Not
recommended unless you know what you are doing.

### References

[Resolving EACCES permissions errors when installing packages globally
\| npm
D...](https://docs.npmjs.com/resolving-eacces-permissions-errors-when-installing-packages-globally#reinstall-npm-with-a-node-version-manager)

[node.js - Error: EACCES: permission denied, access
\'/usr/local/lib/node~modul~...](https://stackoverflow.com/questions/48910876/error-eacces-permission-denied-access-usr-local-lib-node-modules)

## [DONE]{.done .DONE} \*How to use scp and rsync [[terminal]{.smallcaps}]{.tag tag-name="terminal"} {#how-to-use-scp-and-rsync export_file_name="scp_and_rsync" export_hugo_custom_front_matter=":summary There are two options to transfer files between computers using ssh" export_date="<2020-10-29 jue>"}

When managing a remote server via ssh, sometimes you want to move files
between your computer and the server. Two command line options are `scp`
and `rsync`.

> Update (2020-11-06): scp is in the way of being deprecated for several
> reasons, see more
> [here](https://lwn.net/SubscriberLink/835962/ae41b27bc20699ad/).

### scp

`scp` comes by default in all Unix systems (and Windows). It securely
transfers files between two computers. It\'s used like:

`scp -options <source> <target>`

Where source and target can be in the local file system or in the remote
device with the form `<user>@<address>:<full_remote_path>`.

To transfer a file to a remote host it would look like:

`scp ./file.zip remoteuser@192.168.1.44:~/Downloads/newfile.zip`

You can do it recursively with `-r`, change the port with `-P`. To see
all options, use `-h` or check the
[*manpage*](http://manpages.ubuntu.com/manpages/bionic/man1/scp.1.html).

### rsync

Another option, much more versatile and powerful, is `rsync`. Rsync
offers much more control on the transfer.

Very much like scp, you do:

`rsync -options <source> <target>`

Some of the extra options are [setting the ownership of the file in the
destination](https://serverfault.com/a/656494), with
`-og --chown=<user>:<group>`, or delete files in the destination folder
that have been deleted in the source with the `-delete` option, or
perform a dry-run with `-n` or `--dry-run` to see what rsync would do
without actually doing it. Another detail, to use non-standard ports in
ssh, you\'ll need to add the option `-e 'ssh -p <port-number>'`. To see
more, the [manpage is here](https://linux.die.net/man/1/rsync).

`rsync` comes with algorithm optimizations I\'m the transfer algorithm,
making it faster for complex jobs with multiple files. This makes it
widely used for backups and mirroring. It also has advanced options that
scp doesn\'t have.

Conclusion is, for single or few files in your day to day you can use
any, maybe `scp` will be simpler. For backups or periodic transfers of
many files, `rsync` is the way to go.

### References

-   [use scp
    recursively](https://unix.stackexchange.com/questions/232946/how-to-copy-all-files-from-a-directory-to-a-remote-directory-using-scp)
-   [Use rsync to set user:group ownership in target
    computer](https://serverfault.com/questions/564385/rsync-command-issues-owner-and-group-permissions-doesn%C2%B4t-change/648800)
-   [solve the problem with the password
    prompt](https://superuser.com/questions/994450/rsync-with-ssh-prompting-for-remote-password)
-   [How does scp differ from
    rsync](https://stackoverflow.com/questions/20244585/how-does-scp-differ-from-rsync)
-   [How to Sync Files/Directories Using Rsync with Non-standard SSH
    Port](https://www.tecmint.com/sync-files-using-rsync-with-non-standard-ssh-port/)
-   [backup - How do I make rsync delete files that have been deleted
    from the
    sou...](https://askubuntu.com/questions/476041/how-do-i-make-rsync-delete-files-that-have-been-deleted-from-the-source-folder)

## [DONE]{.done .DONE} \*Aliases in Ubuntu Bash and Windows PowerShell [[terminal]{.smallcaps}]{.tag tag-name="terminal"} {#aliases-in-ubuntu-bash-and-windows-powershell export_hugo_custom_front_matter=":summary Save time when using the terminal with terminal shortcuts! (aka aliases)" export_date="<2020-10-30 vie>" export_file_name="aliases"}

When you use the terminal, it\'s easy that you find yourself typing
certain commands again and again (*ehem*
`sudo systemctl restart whatever.service` or `sudo apt update`). In
those cases, wouldn\'t it be nice to have shortcuts? The answer are
aliases.

An alias is a substitution. Instead of typing `ls -l`, you can shorten
it to `ll`. And what about `sshh` instead of `ssh -options user@host`?
Sweet.

### Add aliases in Ubuntu Bash

If you check your home folder, `.bashrc` will be among the *dotfiles*.
It is an shell script that Bash runs every time it\'s opened. It
contains configurations and user environment variables that should be
available every time you open the terminal. You can place aliases here,
or use `.bash-aliases`, but make sure that the code below is included in
`.bashrc`:

``` bash
# Alias definitions.
# You may want to put all your additions into a separate file like
# ~/.bash_aliases, instead of adding them here directly.
# See /usr/share/doc/bash-doc/examples in the bash-doc package.

if [ -f ~/.bash_aliases ]; then
. ~/.bash_aliases
fi
```

If the file doesn\'t exist and you create it, don\'t forget to make it
executable.

Anyway, add your own alias in the form `alias <alias>=<command>`:

``` shell
alias ll='ls -l'
alias update='sudo apt update'
alias upgrade='sudo apt upgrade'
```

To be able to use them immediately, run `source ~/.bashrc` or else,
close and open the terminal again.

[In this
page](https://www.cyberciti.biz/tips/bash-aliases-mac-centos-linux-unix.html)
there are some nice ideas, I took some for my collection.

### Add aliases in Windows 10 PowerShell

Windows PowerShell has a variable called `$profile` that stores the path
to the profile active for the current session. This profile contains
custom configuration, like the default path, size of the window etc. You
can add aliases as well. To edit its content, just type:

`notepad $profile`

Aliases in PowerShell have the form `function <alias> {<command>}`. An
example:

`function .. { cd .. }`

To make the changes effective in the same session, run `. $profile`.

### References

-   [How to create aliases in
    PowerShell](https://stackoverflow.com/a/50954674/12934770)
-   [PowerShell
    aliases](https://www.sconstantinou.com/powershell-aliases/)
-   [How does .bashrc
    work](https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work)
-   [bash - How to escape single quotes correctly creating an alias -
    Unix & Linux](https://unix.stackexchange.com/a/159592)

## [DONE]{.done .DONE} \*Prevent brute force attacks with fail2ban [[linux]{.smallcaps}]{.tag tag-name="linux"}¬†[[server]{.smallcaps}]{.tag tag-name="server"} {#prevent-brute-force-attacks-with-fail2ban export_hugo_custom_front_matter=":summary Today I reviewed the server fail2ban configuration" export_date="<2020-11-12 jue>" export_file_name="fail2ban"}

[Fail2ban](https://manpages.debian.org/experimental/fail2ban/fail2ban.1.en.html)
**scans** log files and **bans** IP\'s that show certain behaviours:
failed logins, seeking vulnerabilities etc. To do this, it updates the
firewall, rejecting any request for a period of time. It comes with
predefined filters (regex conditions that match log entries from other
apps) but you can create your own.

To tell fail2ban which log to use for each filter, you create (or
enable) **jails**. When a log entry matches the filter, an *action* is
performed. You tell fail2ban a default action, like: ban the IP for 1h
if they fail 3 times in less than 5 minutes. Then you can tweak the
actions for each jail.

Let\'s see how it works:

### How to configure fail2ban

The configuration is done in two files: `fail2ban.conf` and `jail.conf`.
The first one contains general configurations. Each one has a pretty
good explanation with the possible values in the comments, so set it up
to your liking.

`jail.conf` where jails and actions are configured. To avoid problems
and be able to backup, copy `jail.conf` to `jail.local`.

> The jail.conf file contains a basic configuration that you can use as
> a starting point, but it may be overwritten during updates. Fail2ban
> uses the separate jail.local file to actually read your configuration
> settings.

The configuration file is organized in sections. Locate the `[Default]`
section, this will contain options that will affect all jails. Later,
you can change specific values in each jail. This are the main options
in the default section:

``` conf
[Default]
bantime  = 60m
findtime = 5m
maxretry = 3
enabled = false
banaction = iptables-multiport
# "filter" defines the filter to use by the jail.
#  By default jails have names matching their filter name
filter = %(__name__)s[mode=%(mode)s]
port = 0:65535
```

With this your jails will be disabled by default and the filter used
should be named as the jail. IP\'s that match the filter 3 times in 5
minutes, will be blocked in iptables for 1 hour, all ports.

1.  Set up *jails*

    Fail2ban brings predefined jail. They are defined like sections:
    `[jail-name]`, with options. An example would be:

    ``` conf
    [sshd]
    # To use more aggressive sshd modes set filter parameter "mode" in jail.local:
    # normal (default), ddos, extra or aggressive (combines all).
    # See "tests/files/logs/sshd" or "filter.d/sshd.conf" for usage example and details.
    enabled = true
    mode   = normal
    port    = ssh
    logpath = %(sshd_log)s
    ```

    This monitors login attempts in ssh (if using public key, set mode
    to `aggressive`, as mentioned in this
    [issue](https://github.com/fail2ban/fail2ban/issues/2361#issuecomment-479703314)).
    It overrides the port, banning the IP only in port 22. In this case,
    logpath is a variable, but it can be a path with wildcards, see
    below. The filter, as it is not specified, is in
    `filter.d/sshd.conf`.

    There are a bunch of predefined ones for Apache, Nginx, and others,
    and you can find people have made custom ones for any other web
    applications.

    This can be a typical custom jail:

    ``` conf
    [jellyfin]
    enabled = true
    logpath = /var/log/jellyfin/jellyfin*.log
    ```

    The logs are specified in `logpath`, and it\'s enabled. The filter,
    in `filter.d/jellyfin.conf`:

    ``` conf
    [Definition]
    failregex =\[INF\]\sAuthentication\srequest\sfor\s\".*?\"\shas\sbeen\sdenied\s\(IP:\s\"<HOST>\"\).
    ignoreregex =
    ```

    It contains `failregex`, a regular expression that matches a failed
    login, and `ignoreregex`, in case you want to ignore certain log
    entries that match failregex. I don\'t. The key part here is the
    variable `<HOST>`. This tells Fail2ban where the IP is localted.
    There are others, like `<IP4>`, `<SUBNET>` or `<DNS>`.

    Here are some other options you can add in a rule to override the
    default values:

    ``` conf
    maxretry = 2
    port = http, https
    bantime = 48h
    action = iptables-allports
    filter = filter-bots
    ```

    There are additional options for the actions, jails and filters. I
    found this
    [manual](https://manpages.debian.org/experimental/fail2ban/jail.conf.5.en.html#FILTER_FILES_(filter.d/*.conf))
    helpful to understand some of them.

    Once you have set up the jails you want, restart the service. Now
    there\'s a daemon that will ban IP\'s when they try to do something
    weird.

    The two main things that I do from time to time are:

### Check bans

The official way of doing this is checking each jail with:
`fail2ban-client status` to check the active jails and
`fail2ban-client status <jail-name>` to see the details. However, I
found [this answer](https://askubuntu.com/a/893108/1138951) with a nicer
command that shows everything in one go (needs sudo):

`fail2ban-client status | grep "Jail list:" | sed "s/ //g" | awk '{split($2,a,",");for(i in a) system("fail2ban-client status " a[i])}' | grep "Status\|IP list"`

I have this in an [alias](file:///blog/aliases): `fail2ban`. I had to
escape all the `"` and `$` characters, but it works:

``` bash
alias fail2ban="sudo fail2ban-client status | grep \"Jail list:\" | sudo sed \"s/ //g\" | sudo awk '{split(\$2,a,\",\");for(i in a) system(\"sudo fail2ban-client status \" a[i])}' | grep \"Status\|IP list\""
```

### Unban an IP

To unban an IP (v0.8.8 or later) use:

`fail2ban-client set <jail-name> unbanip <ip-address>`

### References

-   [jail.conf(5) --- fail2ban --- Debian experimental --- Debian
    Manpages](https://manpages.debian.org/experimental/fail2ban/jail.conf.5.en.html)
-   [firewall - How do you view all of the banned IP\'s for Ubuntu 12.04
    via the
    co...](https://askubuntu.com/questions/487740/how-do-you-view-all-of-the-banned-ips-for-ubuntu-12-04-via-the-command-line)
-   [Fail2Ban: how to unban IPs that are
    blocked?](https://bobcares.com/blog/fail2ban-unban-ip/)
-   [Fail2Ban SSH Nginx Persistent Bans Ubuntu
    16.04](https://ubuntu101.co.za/security/fail2ban/fail2ban-persistent-bans-ubuntu/)
-   [Evitar ataques de fuerza bruta en Nginx con Fail2ban -
    ochobitshacenunbyte](https://www.ochobitshacenunbyte.com/2020/01/15/evitar-ataques-de-fuerza-bruta-en-nginx-con-fail2ban/)
-   [How to secure and protect Nginx on Linux with
    Fail2ban](https://chlee.co/how-to-secure-and-protect-nginx-on-linux-with-fail2ban/)
-   [How to harden a server with
    fail2ban](https://www.a2hosting.com/kb/security/hardening-a-server-with-fail2ban)
-   [nginx - Fail2Ban blocking behaviours depending on the status code -
    Server
    Fault](https://serverfault.com/questions/849854/fail2ban-blocking-behaviours-depending-on-the-status-code)

## [DONE]{.done .DONE} \*Lessons from Brawlhalla I [[personal]{.smallcaps}]{.tag tag-name="personal"} {#lessons-from-brawlhalla-i export_file_name="lessons_brawlhalla" export_date="<2020-11-13 vie>" export_hugo_custom_front_matter=":summary What trying to master a game is teaching me"}

Recently I gave myself a goal: become a good player of Brawlhalla.

### Motivation

While I enjoy playing videogames, I\'ve never been good at them.
Specially combat ones. When playing with friends, I tend to loose.

And I found I get frustrated fast. I don\'t like to get beaten and I
want to run away from that unpleasant feeling. I realized this was
bigger than that. Not only happens with videogames, I don\'t like
physical exercise either. And when I\'m learning something new and I
just can\'t grasp the concepts, *arrg*. If I\'m not naturally good at
something, I don\'t like it. Even if at the end of the day I manage to
do everything, why does it have to be so annoying?

So, as videogames is something I enjoy, I could work on my frustration
here. That will help me in other areas and I\'ll become a better player
(something that I also want).

I chose Brawlhalla for no special reason. Like I said, I\'m not a gamer.
I feel it is a casual game, it\'s easy to get, you can play offline and
online... and it\'s free.

### What I\'ve learned so far (work in progress)

-   Learn the controls is the easy part.
-   The best defense is the attack
-   Dodge!
-   To get better you need to practise
-   Whenever you feel you\'ve had enough, play more

&nbsp;

**Learn the controls**

In Brawlhalla, the controls are not complicated: light and heavy
attacks, pick up and throw gadgets, dodges and jumps. Each character has
different attacks, so you will want to try them. However, you will end
up having one or two that are your main ones.

The Brawlhalla wiki page has all the details on [combat
mechanics](https://brawlhalla.fandom.com/wiki/Combat_mechanics).

**The best defense is the attack**

Only by avoiding attacks you won\'t win. It will only defer your death.
So get out there and kick some ass.

**Dodging**

The good use of this single button is key to become a good player.
Reckless attacks will make you deadly, but if you don\'t dodge, you will
end up with more damage taken than done. And all the time you spent
jumping back to the platform and dying is precious.

**To get better you need practise**

This is the advice that you\'ll see everywhere, because it\'s true. You
can refine your game only by playing and watching others play.

**Whenever you feel you\'ve had enough, play more**

I\'ve always said that I have little willpower. I get
tired/bored/frustrated fast, specially if I\'m loosing. But the only way
to progress is keep pushing. And is not me saying it: [mental
toughness](https://en.wikipedia.org/wiki/Mental_toughness) (or grit) can
predict success in sport, education and the workplace. Overcoming
frustration in various forms is key to success in whatever field. So
whenever I\'m ready to quit, I play a little more.

### Conclusions

-   I\'m definitely getting better, but I still have a long way to go
-   Doing work in other areas (physical exercise, learning...) will also
    have a positive effect

## [DONE]{.done .DONE} \*Njalla DNS Record Updater [[linux]{.smallcaps}]{.tag tag-name="linux"}¬†[[project]{.smallcaps}]{.tag tag-name="project"} {#njalla-dns-record-updater export_file_name="njalla_dns_updater" export_date="<2020-11-14 s√°b>" export_hugo_custom_front_matter=":summary Keep your domain pointing to your server when you have a dynamic IP"}

I\'m using [Njalla](https://njal.la/) as my domain provider. As my IP
changes from time to time, I need to keep my domain pointing to my
server. You can do that with services like DynDNS or No-IP, or you can
update the IP address directly in Njalla.

I made a Javascript job that checks the current public IP of where is
running (my server) and updates the DNS records of the domain. This
script is scheduled in crontab, so it does it periodically in case my IP
changes. It\'s in GitHub:
[njalla-dns-updater](https://github.com/kepair/njalla-dns-updater).

> Update at \<2021-01-19 mar>
>
> Njalla already [supports dynamic DNS](https://njal.la/docs/ddns/).
> Create records with the type *Dynamic* and you\'ll be able to update
> the record with a simple `curl` command.
>
> Bundle all your records in a bash script and schedule it in crontab as
> shown below.

### Requirements:

-   Node must be installed (check with `node -v`)

### Steps:

1.  Clone this repo:

```{=html}
<!-- -->
```
    git clone https://github.com/kepair/njalla-dns-updater.git

1.  Install dependencies
    ([njalla-dns](https://www.npmjs.com/package/njalla-dns) as an API
    wrapper and [public-ip](https://www.npmjs.com/package/public-ip) to
    get your IP):

```{=html}
<!-- -->
```
    cd njalla-dns-updater/
    npm install

1.  Change the name of `credentials.txt.example` to `credentials.txt`
    file and write your user and password there. At this point you can
    test the script by running:

```{=html}
<!-- -->
```
    node ./main.js $PWD

*\$PWD returns the current directory.*

*Note: `cronjob.sh` uses `dirname "$0"` as it returns the directory of
the script. This way the script can be executed from any folder.*

1.  Make sure `cronjob.sh` executable by your user:

```{=html}
<!-- -->
```
    sudo chmod u+x cronjob.sh

1.  Create a cron job that runs this every X minutes:

```{=html}
<!-- -->
```
    crontab -e

To add a job that runs every hour:

    0 * * * * * <path-to>/njalla-dns-updater/cronjob.sh

This script updates all **A** records from all domains because it was
what I needed at the moment.

If you want you update all records from only certain domains, comment
line 17 (`const domains = await getDomains();`) and uncomment line 5
(`//const domains = ["domain1.com","domain2.net"]`) and update your
domains accordingly.

### References

-   [romualdr/node-njalla-dns](https://github.com/romualdr/node-njalla-dns)

## [DONE]{.done .DONE} How to Take Smart Notes [[intelligence]{.smallcaps}]{.tag tag-name="intelligence"} {#how-to-take-smart-notes export_file_name="how_to_take_smart_notes" export_date="<2020-11-22 dom>" export_hugo_custom_front_matter=":summary The key to good and effitient writing lies in the intelligent organization of ideas and notes."}

  ------------- --------------------------------------------------------
                
  **Title:**    [How to Take Smart Notes](https://takesmartnotes.com/)
  **Author:**   S√∂nke Ahrens
  ------------- --------------------------------------------------------

This was a good read, I\'ve learned a lot, it has motivated me to write
and until a certain extent, I see some results.

The book focuses in how we learn, the biases and limitations our brain
has when we think, and how writing helps us overcome many of those. From
time to time there are references to [Niklas
Luhmann](https://en.wikipedia.org/wiki/Niklas_Luhmann), a sociologist
that developed a particular note-taking method called *Zettelkasten*.

Although sometimes the author seems to treat Luhmann as some prophet and
*Zettelkasten* the way to heaven, it\'s easy to just take those parts as
just examples. I really enjoyed this book and I would recommend it to
anyone interested in improving their writing or their thinking.

## [DONE]{.done .DONE} I built a remote switch with a Raspberry and Sapper [[project]{.smallcaps}]{.tag tag-name="project"}¬†[[web]{.smallcaps}]{.tag tag-name="web"} {#i-built-a-remote-switch-with-a-raspberry-and-sapper export_file_name="heating_remote" export_date="<2020-11-28 s√°b>" export_hugo_custom_front_matter=":summary My boiler doesn't have a thermostat, so I had to manually switch it on or off. I had a Raspberry Pi 3 and a relay laying around, so I made a remote switch to control it from my phone."}

The boiler at home doesn\'t have a thermostat or any way of controlling
it. I need to switch it on or off manually, from the boiler. It is not
old or anything, it just doesn\'t have a thermostat attached.

It works the following way:

1.  The boiler has a heating-mode. If off, then nothing happens. If on,
    the heating is controlled by an additional switch.
2.  This switch is usually a thermostat. If the current temperature is
    below the target temperature, the switch is closed and the heating
    is on. When the target temperature is reached, the switch opens,
    turning off the heating.

![This is where the thermostat would be. Instead, it is short-circuited,
which keeps it on \"forever\"](../img/thermostat.jpg){class="center"}

What I had to do is turn on and off the heating mode manually. But I had
a Raspberry Pi 3 and a relay laying around, so I made a remote switch.
This replaces the thermostat, so now I can leave the heating mode always
on, and control it via the Raspberry. The circuit is simple and looks
like this:

![The relay is connected to ground, 5V and a signal. In my case it is
GPIO4.](../img/rpi3-relay.png){class="center   "}

Using the standard template from
[Sapper](https://sapper.svelte.dev/docs#What_is_Sapper) I created a web
app with two parts:

-   An index page with a simple button to switch the relay on and off.
-   A `/relay.json` endpoint that supports two HTTP methods: GET gives
    the current state of the relay and POST changes the state.that has a
    button that makes a request to an endpoint. There, using the
    [onoff](https://www.npmjs.com/package/onoff) npm library, I can
    control the digital pins from the Raspberry:

``` javascript
const Gpio = require('onoff').Gpio; // Gpio class
const relay = new Gpio(4, 'out'); // Export GPIO4 as an output

//Change the state of the relay
export function post(req, res) {
    res.writeHead(200, {
        'Content-Type': 'application/json'
    });
    relay.writeSync(1- relay.readSync());
    res.end(JSON.stringify({ state: relay.readSync() }));
} 

//Read the state of the relay
export function get(req, res) {
    res.writeHead(200, {
        'Content-Type': 'application/json'
    });
    res.end(JSON.stringify({ state: relay.readSync()}));
} 
```

The code is in [GitHub](https://github.com/kepair/heating-remote), and
the page looks like this:

![A nice retro looking gifs makes it easier to see whether it\'s on or
off.](./demo.gif){class="center"}

It\'s very simple, there\'s no temperature control or anything, but now
I don\'t have to get up every time I want to turn it on/off. This also
opened the door to programmatically turn the heating on and off. Using
[crontab](https://man7.org/linux/man-pages/man5/crontab.5.html) I can
set the heating to turn on using a simple `wget` command:

``` shell
30 7 * * * wget --post-data=test 192.168.1.33:3000/relay.json >/dev/null 2>&1
```

`30 7 * * *` is a crontab expression to execute this job everyday at
7:30am. The wget command uses the `--post-data` option, which makes the
HTTP request a POST request. I don\'t need any particular data, so
`test` does the job. The final part `>/dev/null 2>&1` discards any
output from the command.

## [DONE]{.done .DONE} A New Theory of Western Civilization [[history]{.smallcaps}]{.tag tag-name="history"} {#a-new-theory-of-western-civilization export_file_name="theory_western_civilization" export_date="[2020-12-05 s√°b]" export_hugo_custom_front_matter=":summary Could a marriage policy first pursued by the Catholic Church a millennium and a half ago explain what made the industrialized world so powerful‚Äîand so peculiar?"}

  ------------- ----------------------------------------------------------------------------------------------------------------------------------
                
  **Title:**    [A New Theory of Western Civilization](https://www.theatlantic.com/magazine/archive/2020/10/joseph-henrich-weird-people/615496/)
  **Author:**   Judith Shulevitz
  ------------- ----------------------------------------------------------------------------------------------------------------------------------

This article summarizes the ideas presented in this book: *[The WEIRDest
People in the World: How the West Became Psychologically Peculiar and
Particularly
Prosperous](https://www.goodreads.com/book/show/51710349-the-weirdest-people-in-the-world)*.
The author proposes that the marriage policy of the Catholic Church
played a fundamental role in the development of Western culture.

Kinship \[fn:2\] is a fundamental relationship of man and many other
living beings. The way in which we relate at the family level is
projected to all other relationships. On a collective level it forms
tribes, kingdoms or corporations. On the individual level it influences
how we see and relate to strangers.

But like everything else, it is a construct. And it has changed over
time, transforming relationships and institutions as well.

In the past, [marriages took place between people close to the
family](https://es.wikipedia.org/wiki/Endogamia#Causa). Bonding with
cousins and even siblings was the norm. This type of kinship formed
institutions based on blood: tribes, kingdoms and dynasties.

The author argues that the Church changed all this. It introduced norms
that prohibited relationships with close relatives. This pushed them to
leave the family nucleus to look for a partner. And this more nuclear
kinship promoted alliances, creating cities, and catapulting Western
society.

However,
[others](https://elpais.com/elpais/2016/10/13/buenavida/1476373768_793920.html)
say that men already knew the benefits of marrying strangers **before**
the Church existed.

What I do agree with is that whatever the cause, kinship has changed
over time. And this has affected how our institutions are: from being
blood-based to voluntary associations.

This article has created more questions than answers really:

-   When did humans start to see the problems of close endogamy
    (brothers/sisters, parents/childrens and even cousins)?

```{=html}
<!-- -->
```
-   Endogamy also brings some advantages (integrated members, closer
    relationships, solidarity...) that have kept [a
    bunch](https://en.wikipedia.org/wiki/Endogamy) of tribes, ethnics
    groups and populations alive until today. How have they managed? Are
    those advantages lost in an exogamous society?

```{=html}
<!-- -->
```
-   Until which extent societies and institutions are projections of
    kinship?

```{=html}
<!-- -->
```
-   How different is the conception of the self and the rest world of
    two people raised in different cultures? How do I see family,
    friends and strangers compared to someone from a different culture?

\[fn:2\] Link by blood, affinity, adoption, marriage, or other stable
relationship of affection analogous to this one. ([Kinship -
Wikipedia](https://en.wikipedia.org/wiki/Kinship))

## [DONE]{.done .DONE} D√≠as de Ira - Exhibition about Helios G√≥mez [[art]{.smallcaps}]{.tag tag-name="art"} {#d√≠as-de-ira---exhibition-about-helios-g√≥mez export_file_name="days_of_ire" export_date="<2020-12-06 dom>" export_hugo_custom_front_matter=":summary Helios G√≥mez (Sevilla, 1905 - Barcelona, 1956), Sevillian, Romany gypsy and Barcelonan, participated in some of the most interesting European creative networks of his time."}

There was a [free exhibition at Virreina
Palace](https://ajuntament.barcelona.cat/lavirreina/en/exhibitions/days-ire-libertarian-communism-flamenco-romanies-and-avant-garde-realism/490)
about this artist. Sounds like a plan to me.

Helios G√≥mez (1905, Sevilla -1956, Barcelona) was a Spanish painter and
poet. He was an anarchist-syndicalist and an activist, creating
syndicates, magazines and posters.

His work is avant-garde, striking and political.:

```{=org}
#+ATTR_HTML: :class center
```
```{=org}
#+CAPTION: Evacuaci√≥n (1837). It was exposed in the Spanish pavilion of the Universal Exposition of Paris of 1937. Gypsies flee from a burning city during the Spanish Civil War (1936-1939).
```
[![](../img/evacuacion-helios-gomez.jpg)](https://www.museunacional.cat/sites/default/files/145177-000_004810.jpg)

He travelled through Europe to meet other artists from the avant-garde
movement, specially German expressionists. Works like [The
Idea](https://www.fundacionendurance.com/wp-content/uploads/2015/masereel/idea.pdf   )
(1920) by Frans Masereel were the kind of art that inspired Helios. In
Berlin he published *D√≠as de Ira* (1930), 23 drawings and poems about
the political situation in Spain.
[Here](http://www.heliosgomez.org/heliosgomez-carpetadiasdeira.htm) you
can see 4 of them.

He also did great works on graphics:

```{=org}
#+ATTR_HTML: :class center
```
```{=org}
#+CAPTION: L'Opini√≥ (1936). Front page of the magazine /L'Opini√≥/, a publication that ran from 1928 to 1934.
```
[![](../img/lopinio-helios.jpg)](https://www.museunacional.cat/sites/default/files/250449-000_083228.jpg)

Without a doubt, he was a man moved by his ideas. His fight for justice
took him to many places, and his art shows all of this.

## [DONE]{.done .DONE} Server-side web analytics [[linux]{.smallcaps}]{.tag tag-name="linux"}¬†[[server]{.smallcaps}]{.tag tag-name="server"} {#server-side-web-analytics export_file_name="server_side_analytics" export_date="<2020-12-14 lun>" export_hugo_custom_front_matter=":summary Use your server logs to create some web analytics"}

I want to know about the visits in my blog, but I don\'t want to rely on
another site to see this. What I want is something simple to check now
and then to see if anyone actually comes here. And who knows better than
the server how many times the page has been requested?

When accessing a page, the browsers shares the OS, the browser, the
previous website (if you clicked a link), the link it wants to access
and the IP. Well, that\'s a basic request, if there\'s any cookie,
service-worker or tracker the list is longer. But I\'m fine with that
information.

The good thing about server side analytics is that it\'s not affected by
ad-blockers. The bad thing is that bots and crawlers (ignored by client
side analytics) are visible here. But there\'s ways to keep them down.

So I tried server side analytics. There are a few tools, some very
90\'s, like [AWStats](https://awstats.sourceforge.io/), or very
complicated, like [Snowplow
Analytics](https://snowplowanalytics.com/server-side/). But then
there\'s [GoAccess](https://goaccess.io/), which I find elegantly
simple.

GoAccess reads log files and creates visual reports via console or as
HTML like the one in [/analytics](file:///analytics/), and the HTML can
even be real-time. I just need a static report, so using crontab I set
up GoAccess to run every hour. It reads the logs, generates the report
and places it in the web server.

```{=org}
#+ATTR_HTML: :class center
```
```{=org}
#+CAPTION: HTML report generated by GoAccess for the traffic on this page
```
[![](../img/goaccess-small.png)](../img/goaccess-full.png)

To set everything I followed [this
blog](https://retifrav.github.io/blog/2020/05/20/visitors-analytics-with-goaccess/).
It shows how to install GoAccess, edit the configuration file and make
sure Nginx is writing proper logs.

At the end I this entry in my crontab:

``` shell
1 * * * * ~/goaccess/webanalytics.sh
```

And this is the script:

``` shell
#!/bin/bash

/bin/zcat -f /var/log/nginx/pedroir.nz.access.log* | /usr/bin/goaccess - -p /etc/goaccess.conf -o /var/www/pedroir.nz/analytics/index.html
```

`zcat` prints current and compressed logs, then GoAccess will parse them
and create the report and place it in the website.

Later, to tune the configuration, the [manual
page](https://goaccess.io/man#synopsis) is quite nice. My configuration
is currently:

``` shell
time-format %H:%M:%S
date-format %d/%b/%Y
log-format COMBINED #Nginx default
html-prefs {"theme":"bright","perPage":5,"layout":"widescreen","showTables":true,"visitors":\{"plot":{"chartType":"area-spline"}}}
                                                                                                                                  html-report-titl pedroir.nz web analytics
exclude-ip #I don't want to count my visits, and most of them came from few places
anonymize-ip true #I'm not showing th IP's, but if I did, I don't want to expose them publicly. This takes the last digit out.
ignore-crawlers true #Very important, takes away most of the robots
keep-last 7 #show only last 7 days
geoip-database /path/to/dbip-database.mmdb #To be able to show countries of visit
```

There are some more configurations, like panels enabled/ignored, static
file extensions etc that are intuitive enough.

The database to parse the country from the IP is from
[db-ip.com](https://db-ip.com/db/download/ip-to-country-lite), you just
need to download it. IP\'s change, so it\'s a good idea to update it now
and then.

I used the default Nginx logging and that\'s ok for me. If you are
interested to customize your logging, the steps are:

1.  Configure [Nginx
    logging](https://docs.nginx.com/nginx/admin-guide/monitoring/logging/#access_log).
    Check [this
    question](https://stackoverflow.com/questions/37437153/dictionary-variable-in-log-format-nginx)
    to see available variables.
2.  Convert the Nginx log format to GoAccess format using
    [nginx2goaccess](https://github.com/stockrt/nginx2goaccess) and
    update the configuration file accordingly.

That\'s how I\'m doing the analytics, which you can check in
[pedroir.nz/analytics](file:///analytics/). From time to time I run an
offline report, showing IP\'s and everything, to have more details.

## [DONE]{.done .DONE} How this blog works [[meta]{.smallcaps}]{.tag tag-name="meta"}¬†[[project]{.smallcaps}]{.tag tag-name="project"} {#how-this-blog-works export_file_name="hugo_blog" export_date="<2020-12-23 mi√©>" export_hugo_custom_front_matter=":summary Running a blog with Hugo and org-mode"}

So just to recap and document it, this is how I set and maintain this
site.

Using the static site generator [Hugo](https://gohugo.io/), I created a
new project using `hugo new site pedroir.nz` (you can see these same
steps in the [quick-start
guide](https://gohugo.io/getting-started/quick-start/)). This generates
a simple folder structure. Next thing is to set up a theme. In Hugo,
themes control the look and feel and the structures of the page. I\'m
using a modified version of the [Archie
theme](https://github.com/athul/archie).

For the content you just need to throw any [supported
file](https://gohugo.io/content-management/formats/) into the `content/`
folder. The hierarchy you set in the folder will be translated to the
website. I have two types of pages:

-   **Posts**: Files inside the folders `blog` and `notes`. The names
    are then the URL (`njalla_dns_updater.md` for example)
-   **Pages**: The content is placed in `_index.md` files at the desired
    route: `about/_index.md` for the [/about](file:///about/) section
    and `_index.md` for the home page.

Once you are happy, run `hugo` and it will generate your static site
under the `public/` folder. Now you can use any web server to serve the
files. In my case, Nginx does the job.

Hugo comes with nice features that need almost no configuration, like
[RSS](https://gohugo.io/templates/rss/) (see it
[here](https://pedroir.nz/index.xml)) and
[taxonomies](https://gohugo.io/content-management/taxonomies/) or tags
(see them [here)](https://pedroir.nz/tags/).

### Writing posts

I use [org-mode](https://orgmode.org/) in Emacs for to-do lists,
calendar events, reading notes, diaries etc. I also write the blog
there. All posts (published and drafts) are in a single file.

The process starts when I\'m actually working on something. I create a
new section that I use to make notes as I work, saving links, steps and
anything worth keeping somewhere. This way, whenever I want to write a
post, I never start from scratch, as there are already ideas and
important points.

When the post is ready, I use [ox-hugo](https://ox-hugo.scripter.co/) to
translate the org section to a markdown post. The org file and the
section have some properties configured that automatically places the
post in the right place (under `content/blog/`). The org file looks like
this:

``` org
#+title: Blog
#+hugo_base_dir: ~/workspace/pedroir.nz
#+hugo_section: ./blog
#+author: kepair

* Published
** DONE Hello world                                                    :meta:
   :PROPERTIES:
   :EXPORT_FILE_NAME: hello_world
   :EXPORT_DATE: <2020-10-25 dom>
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :summary My first entry in the blog
   :END:

   Hi üëã,

   Being the first post, I'll explain what this site is going to be: a blog.

** DONE Install npm packages glob...
   ...
* Drafts
** Client-side certificates
** Pandoc  
```

The global properties point to the destination and set the author, and
the properties of each post specify the export date, the file name and
the summary.

After exporting, the resulting markdown file looks like this:

``` markdown
+++
title = "Hello world"
author = ["kepair"]
date = 2020-10-25T00:00:00+02:00
lastmod = 2020-12-15T09:33:49+01:00
tags = ["meta"]
draft = false
summary = "My first entry in the blog"
+++

Hi üëã,

Being the first post, I'll explain what this site is going to be: a blog.
```

### Deploying new versions

I used to build and deploy the website manually (using `rsync` to upload
the files to the server) and I created a bash script to do that:

``` bash
#!/bin/bash

echo "Removing old build"
rm -r public/
echo "Building webiste"
hugo --minify
echo "Deploying website"
rsync -rog --rsync-path="rm -r /var/www/pedroir.nz/* && rsync" ~/workspace/pedroir.nz/public/* <my-user>@<my-server>:/var/www/pedroir.nz
echo "Website deployed"
```

Now I use GitHub Webhooks to do all this every time I push any changes
to GitHub. I\'m using [webhook](https://github.com/adnanh/webhook), a
lightweight server where you can configure endpoints that listen to
webhooks. It already has [templates for
GitHub](https://github.com/adnanh/webhook/blob/master/docs/Hook-Examples.md)
and more, where you just have to change a few parameters.

When webhook receives a POST message, it will validate it (to make sure
it comes from a branch of your repository in GitHub) and then it will
run the script that is specified:

> The script uses the template described in [this
> post](https://betterdev.blog/minimal-safe-bash-script-template/), look
> at the bottom for the actual code.

``` bash
#!/bin/bash

set -Eeuo pipefail
trap cleanup SIGINT SIGTERM ERR EXIT

script_dir=$(cd "$(dirname "${BASH_SOURCE[0]}")" &>/dev/null && pwd -P)

msg() {
  echo >&2 -e "${1-}"
}

parse_params() {
  # default values of variables set from params
  force=
  delete=

  while :; do
    case "${1-}" in
    -v | --verbose) set -x ;;
    --no-color) NO_COLOR=1 ;;
    -f | --force) 
        force=-f
        delete=--delete
        ;; 
    -?*) die "Unknown option: $1" ;;
    *) break ;;
    esac
    shift
  done

  args=("$@")
  return 0
}

parse_params "$@"

# My script starts here

# Clean up building directory
rm -r ${force} $script_dir/public/*
# Get last version from GitHub
git pull
# Build Hugo website minify the files
hugo --minify
# Apply new version
rsync -avh $script_dir/public/ /var/www/pedroir.nz/ ${delete}
msg "Website deployed"
# Performing analytics
./webanalytics.sh
msg "Analytics done"
```

The main change is that now I use `rsync` to move the built website to
the public. Rsync by default only creates and updates files. If I use
the `-f` flag, it will delete the files in the destination if they were
deleted in the source folder.

I had some issues making webhook work (the validations were never
passed), and I solved them building it from source.

### Problems & solutions

As I work with Hugo, I need to find fixes, tutorials and solutions to
problems and features I want to introduce. Here I keep a list of them:

**Question**: How to make all external links open in a new page? (adding
`target="_blank"` and `rel="noopener"` to all external links)

**Answer**: From [this
blog](https://agrimprasad.com/post/hugo-goldmark-markdown/). Use render
hooks to modify the HTML that generates from the Markdown. Create a file
at `layouts/_default/_markup/render-link.html` with the following
content:

``` html
<a href="{{ .Destination | safeURL }}"{{ with .Title}} title="{{ . }}"{{ end }}{{ if strings.HasPrefix .Destination "http" }} rel="noopener" target="_blank"{{ end }}>{{ .PlainText }}</a>
```

**Question**: How to use the ampersand character in a link?

**Answer**: Using the same render hook from before. [The
answer](https://discourse.gohugo.io/t/ampersand-character-in-markdown-links/25269/4)
is to use `.PlainText` instead of `.Text` (already implemeted in the
above code).

**Question**: How to make the `git pull` run by webhook work without
having to store the credentials?

**Answer**: Using an SSH key-pair, as mentioned in [this
blog](https://ep.gnt.md/index.php/how-to-automatically-deploy-from-github-to-server-using-webhook/)
(sections *Setting up Git* and *Configure Deployment Key and Webhooks
(GitHub)*.

**Question**: How to add a summary to the post using ox-hugo?

**Answer**: Using the property
`:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :summary Summary goes here` as shown
in [this blog](https://people.umass.edu/weikaichen/post/emacs-ox-hugo/).

**Question**: Links have space after it (noticeable when there\'s a
comma or period jsut after it). Why and how to remove?

**Answer**: There seems to be newlines added when using Goldmark to
parse markdown to HTML. [The
workaround](https://discourse.gohugo.io/t/white-space-issue-in-markdown/25258/11),
add `{{- /* This comment removes trailing newlines. */ -}}` to the file
mentioned before: `layouts/_default/_markup/render-link.html`. The
resulting file:

``` html
<a href="{{ .Destination | safeURL }}"{{ with .Title}} title="{{ . }}"{{ end }}{{ if strings.HasPrefix .Destination "http" }} rel="noopener" target="_blank"{{ end }}>{{ .PlainText }}</a>
{{- /* This comment removes trailing newlines. */ -}}
```

## [DONE]{.done .DONE} Self-replicating software [[intelligence]{.smallcaps}]{.tag tag-name="intelligence"} {#self-replicating-software export_file_name="self_replicating_software" export_date="<2020-12-25 vie>" export_hugo_custom_front_matter=":summary Or what makes software intelligent"}

I\'ve been reading [**G√∂del, Escher,
Bach**](https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach) for a
couple weeks now. It is a book that talks about intelligence: how can we
define it and how something (humans or computers) can posses it. To do
this, the author discusses about logic, mathematics, art, music,
computers, etc... Throughout the chapters, self-reference and loops
appear everywhere and are key in the investigation.

```{=org}
#+ATTR_HTML: :class center
```
```{=org}
#+CAPTION: /Drawing Hands/ by M.C. Escher (1948)
```
[![](../img/drawing-hands.jpg)](https://en.wikipedia.org/wiki/Drawing_Hands)

The book is full of puzzles and though experiments for the reader to
think. As a little exercise, I wanted to write a program that replicated
itself, something like the Escher drawing. The author calls this type of
self-referenced programs *quines*:

> A quine is a computer program which takes no input and produces a copy
> of its own source code as its only output. The standard terms for
> these programs in the computability theory and computer science
> literature are \"self-replicating programs\", \"self-reproducing
> programs\", and \"self-copying programs\".

*[Quine](https://en.wikipedia.org/wiki/Quine_%28computing%29) from
Wikipedia*

After seeing examples in other languages and following some steps from
[this](http://www.madore.org/~david/computers/quine.html) and
[this](http://www.cs.utsa.edu/~wagner/CS2213/quine/quine.html) articles,
I was able to finish a first version:

``` c
#include <stdio.h>

int
main()
{
    char *s1="#include <stdio.h>%c%cint%cmain (void)%c{%c";
    char *s2=" char *s1=%c%s%c;%c char *s2=%c%s%c;%c char *s3=%c%s%c;%c char n='%cn',q='%c',b='%c%c';%c";
    char *s3=" printf(s1,n,n,n,n,n);%c printf(s2,q,s1,q,n,q,s2,q,n,q,s3,q,n,b,q,b,b,n);%c  printf(s3,n,n,n,n,n);%c}%c";
    char n='\n',q='"',b='\\';
    printf(s1,n,n,n,n,n);
    printf(s2,q,s1,q,n,q,s2,q,n,q,s3,q,n,b,q,b,b,n);
    printf(s3,n,n,n,n,n);
}
```

The program has two parts:

-   The `char` variables `s1`, `s2` and `s3` store the code itself. To
    be able to replicate newlines, quotes and backlashes, the variables
    use the tags `%c` and `%s` that later are replaced by the variables
    `n`, `q`, `b`.
-   The 3 `printf()` produce the output. In them we print `s1`, `s2` and
    `s3` replacing the tags with the appropriate characters.

There are other examples using different tricks, like this
[one-liner](https://stackoverflow.com/questions/10238670/c-c-program-that-prints-its-own-source-code-as-its-output)
that leverages compiler assumptions, or another one that [encodes the
program as ASCII
characters](http://www.madore.org/~david/computers/quine.html).

When compiling and executing this, the output is an exact copy of the
original code:

``` shell-session
$ gcc -o quine quine.c   #Compiling the program
$ ./quine                #Running the program
#include <stdio.h>

int
main()
{
    char *s1="#include <stdio.h>%c%cint%cmain (void)%c{%c";
    char *s2=" char *s1=%c%s%c;%c char *s2=%c%s%c;%c char *s3=%c%s%c;%c char n='%cn',q='%c',b='%c%c';%c";
    char *s3=" printf(s1,n,n,n,n,n);%c printf(s2,q,s1,q,n,q,s2,q,n,q,s3,q,n,b,q,b,b,n);%c  printf(s3,n,n,n,n,n);%c}%c";
    char n='\n',q='"',b='\\';
    printf(s1,n,n,n,n,n);
    printf(s2,q,s1,q,n,q,s2,q,n,q,s3,q,n,b,q,b,b,n);
    printf(s3,n,n,n,n,n);
}
$
```

Of course this program has no intelligence, it prints what I programmed
it to print. But it gives a glimpse of something more here.

We consider ourselves intelligent beings, and a big reason is that we
are conscious of our own existence. A system that *inside itself* has a
representation of itself. Self-reference is a key element, more
important even than our mathematical ability.

That\'s why I find this program interesting. One that calculates orbits
of planets, for example, can impress, but is nothing more than complex
mathematical operations. Self-replicating software, however, seem more
*alive*.

## [DONE]{.done .DONE} As we may think - Vannevar Bush [[history]{.smallcaps}]{.tag tag-name="history"}¬†[[science]{.smallcaps}]{.tag tag-name="science"} {#as-we-may-think---vannevar-bush export_file_name="as_we_may_think" export_date="<2020-12-28 lun>" export_hugo_custom_front_matter=":summary At the end of the war, in 1947, this article was published telling scientists where they should focus their efforts"}

  ------------- -----------------------------------------------------------------
                
  **Title:**    [As we my Think](http://web.mit.edu/STS.035/www/PDFs/think.pdf)
  **Author:**   [Vannevar Bush](https://en.wikipedia.org/wiki/Vannevar_Bush)
  ------------- -----------------------------------------------------------------

As Director of the Office of Scientific Research and Development, Dr.
Vannevar Bush coordinated the activities of around six thousand leading
American scientists in the application of science to warfare. Apart of
more accurate bombs and reliable detonators, the office developed radar
technology and early-warning systems. The most secret of them was the
[Manhattan Project](https://en.wikipedia.org/wiki/Manhattan_Project).

In 1947, he published this article saying:

> This has not been a scientist\'s war; it has been a war in which all
> have had a part. The scientists, burying their old professional
> competition in the demand of a common cause, have shared greatly and
> learned much. It has been exhilarating to work in effective
> partnership.

It was important to realize that science was not at the service of war,
or that war was fueled by science, and he addressed these scientists to
say where they should focus their efforts now that the war was over.

His main concern was knowledge. With more and more research being done,
the problem is how to share and navigate the inventions, discoveries and
ideas from all over the world.

**Knowledge *tends to* not take up any space**

On how to *save* knowledge, he claims that photography and typewriters
could produce projectable microfilms that would reduce the Encyclop√¶dia
Britannica to the size of a matchbox. Not far off, today\'s microchips
are capable of storing much more in a format that is then *projected* on
a screen. We have been reducing the size of knowledge consistently: from
clay tablets, to paper, books, microfilms and chips.

**Leave mechanical tasks to the machines**

At that time, calculating machines were already possible, and he sees no
obstacle in creating more complex computers:

> It is readily possible to construct a machine which will manipulate
> premises in accordance with formal logic, simply by the clever use of
> relay circuits. Put a set of premises into such a device and turn the
> crank, and it will readily pass out conclusion after conclusion, all
> in accordance with logical law, and with no more slips than would be
> expected of a keyboard adding machine.

Their goal is for human beings to devote themselves to what makes them
unique: discovering and associating concepts, while machines do the
*dirty work*:

> A mathematician is not a man who can readily manipulate figures; often
> he cannot. He is not even a man who can readily perform the
> transformation of equations by the use of calculus. He is primarily an
> individual who is skilled in the use of symbolic logic on a high
> plane, and especially he is a man of intuitive judgment in the choice
> of the manipulative processes he employs.
>
> All else he should be able to turn over to his mechanism, just as
> confidently as he turns over the propelling of his car to the
> intricate mechanism under the hood.

**Knowledge network map**

But having an infinite number of documents and calculations without
order is like having nothing. The next challenge to be faced is the
organization of such information. To do this, he suggests learning how
the mind works, to apply the same principles to published information:

> When data of any sort are placed in storage, they are filed
> alphabetically or numerically, and information is found (when it is)
> by tracing it down from subclass to subclass. It can be in only one
> place, unless duplicates are used; one has to have rules as to which
> path will locate it, and the rules are cumbersome. Having found one
> item, moreover, one has to emerge from the system and re-enter on a
> new path.
>
> The human mind does not work that way. It operates by association.
> With one item in its grasp, it snaps instantly to the next that is
> suggested by the association of thoughts, in accordance with some
> intricate web of trails carried by the cells of the brain. It has
> other characteristics, of course; trails that are not frequently
> followed are prone to fade, items are not fully permanent, memory is
> transitory. Yet the speed of action, the intricacy of trails, the
> detail of mental pictures, is awe-inspiring beyond all else in nature.

**External brains**

With these premises, in his mind he designs a personal machine that acts
as an external brain. The user enters his thoughts, notes, books, ideas,
photographs with almost no effort. Then, he connects each element with
others, creating a network of interconnected ideas, with paths that
emulate how the mind associates different concepts:

> Consider a future device for individual use, which is a sort of
> mechanized private file and library. It needs a name, and to coin one
> at random, \'\'memex\'\' will do. A memex is a device in which an
> individual stores all his books, records, and communications, and
> which is mechanized so that it may be consulted with exceeding speed
> and flexibility. It is an enlarged intimate supplement to his memory.

Such device is also discussed in [How to Take Smart
Notes](file:///notes/how_to_take_smart_notes). This book talks about
letting the brain do what it does best, while using an external system
(a *memex*) for the rest.

**Conclusion**

Vannevar Bush was truly a visionary in many ways.

> The applications of science have built man a well-supplied house, and
> are teaching him to live healthily therein. They have enabled him to
> throw masses of people against another with cruel weapons. They may
> yet allow him truly to encompass the great record and to grow in the
> wisdom of race experience. He may perish in conflict before he learns
> to wield that record for his true good. Yet, in the application of
> science to the needs and desires of man, it would seem to be a
> singularly unfortunate stage at which to terminate the process, or to
> lose hope as to the outcome.

He was well aware of how science can be used to kill, however he didn\'t
doubt that science could also bring progress and improvements.

Today we have managed to have all the knowledge available with a click
thanks to Internet, but we still use hierarchies that are not intuitive
for our minds. I\'m glad to see developments in this regard, as [graph
databases](https://en.wikipedia.org/wiki/Graph_database), [Hopfield
networks](https://en.wikipedia.org/wiki/Hopfield_network), and open
projects like
[Athens](https://github.com/athensresearch/athens/blob/master/VISION.md).
All this is bringing technology to a whole new level.

## [DONE]{.done .DONE} PocketBook got it almost right [[linux]{.smallcaps}]{.tag tag-name="linux"}¬†[[project]{.smallcaps}]{.tag tag-name="project"} {#pocketbook-got-it-almost-right export_file_name="pocketbook_tweaks" export_date="<2021-02-26 vie>" export_hugo_custom_front_matter=":summary The world of eReaders is full of absurd lock-ins. Open software and the community come to the rescue."}

I got into the world of the eReaders this Christmas.

One of the things I noticed when choosing an eReader is the few brands
out there compared to phones or laptops, and the lock-ins they have. In
Wikipedia there\'s a [eReader comparison
table](https://en.wikipedia.org/wiki/Comparison_of_e-readers) with
features, compatible formats etc. I didn\'t need anything fancy, but I
wanted it to be as open as possible have a decent screen size and a nice
design.

At the end I got a [PocketBook InkPad
3](https://www.pocketbook-int.com/ge/products/pocketbook-inkpad-3) and
I\'m really enjoying it. This of course after I tweaked a few things.

> I\'m not responsible of any undesired consequences. These changes are
> just suggestions based on my experience.

### Changing the start page of the browser

I noticed that when you open the browser, the page
[start.obreey.com](https://start.obreey.com) opens up:

![<http://start.obreey.com> -
Uh?](../img/start.obreey.com.png){class="center"}

This page just redirects you to `www.google.com` after a second. On the
side, it also loads Google Analytics. It looked fishy, but turns out
that\'s standard behavior! Some people have asked about it
([here](https://papierlos-lesen.de/der-langerwartete-pocketbook-touch-hd-3-im-ausfuehrlichen-test-66040/)
and
[here](https://papierlos-lesen.de/firmware-5-14-fuer-pocketbook-touch-lux-2-und-3-59480/),
both in German) and support confirmed that it\'s not possible to change
it. [Obreey](http://obreey-products.com/en/) is actually the company
that did the software for the PocketBooks.

I can imagine this is the way they found to have analytics on the
browser application usage. But I find it clumsy and intrusive. eReaders
are not the fastest devices, and this an extra second (best case
scenario) until I can start typing. So I kept digging around some more
to see if I could change this.

This is how you can do it using Linux:

-   Connect the PocketBook to a PC
-   Open the file system/config/global.cfg
-   Search for \"\", change the URL to the one you want
-   Save, close, unplug it and reboot the PocketBook

![Thanks but no thanks](../img/global.cfg.png){class="center"}

If you are using Windows, you won\'t see the `system` folder. I would
recommend setting up a Linux virtual machine with VirtualBox. You\'ll
thank me later.

Credits to [Papierlos
Lesen](https://papierlos-lesen.de/ueber-papierlos-lesen/) that also
describes the method. I actually found it in a post on
[Malbergweich](https://malbergweich.de/berichte/buecherei/anleitung-zum-pocketbook-inkpad-3.html),
a website of small German town. Internet just amazes me.

### Syncing a folder with WebDav

The next step was synchronizing eBooks. This model comes with four
options:

-   Dropbox PocketBook
-   PocketBook Cloud
-   Send-to-PocketBook
-   Connect it to a PC via USB

I already have my own cloud, so I don\'t want to register in any of
these services. On the other hand, I want to be able to sync my books
via WiFi, so, what can be done here?

Fortunately, PocketBook eReaders run on Linux, and there are people
writing apps, and they are happy to share them. The apps are Linux bash
scripts with using an `.app` extension instead of `.sh`. I found a few
that can synchronize folders using WebDav protocol. Most of them are
focused on Nextcloud, but I found this generic one:

-   [PocketBook Scripts -
    GitHub](https://github.com/Loriowar/pocketbook_scripts): Generic
    webdav sync. It brings the files from the remote folder to the
    eReader. Doesn\'t delete local files if they have been deleted from
    the remote folder.

> [WebDav](https://www.cloudwards.net/what-is-webdav/) is an Internet
> protocol, like HTTP. While HTTP is used to access files (websites) in
> read-only mode, WebDav allows to access and modify those remote files
> in a secure way. A lot of tools support this protocol, among them,
> [Owncloud](https://owncloud.com/), [NextCloud](https://nextcloud.com/)
> and [Seafile](https://www.seafile.com/en/home/) (self-hosted private
> clouds). I\'m currently using Seafile.

To install these apps you need to:

-   Connect it to the PC via USB

-   Download the app file. Make sure the extension of the file is
    `.app`.

-   Copy it inside `applications/`

    After doing this, I can sync the books without having to put them
    somewhere else. Nifty.

    Other apps worth to mention are [GitHub -
    OliverHaag/pocketbook-apps](https://github.com/OliverHaag/pocketbook-apps)
    and [GitHub -
    JuanJakobo/Pocketbook-Nextcloud-Client](https://github.com/JuanJakobo/Pocketbook-Nextcloud-Client).
    The first one is similar to the one I\'m using, but couldn\'t make
    it work. The second is a more complex app that works with Nextcloud
    only. Totally recommended if that\'s your case.

### Rooting the device

Finally, you can root the eReader. This was unnecessary for me, I don\'t
need root for anything, but why not. I don\'t like when you buy a device
but they don\'t give you full access to it, like with phones or Internet
routers.

Fortunately again, someone has taken the time to investigate, write and
share:

[PB62x/740: root + sshd, ftpd, smbd, iptables, usbnet (needs testers) -
Mobile...](https://www.mobileread.com/forums/showthread.php?t=325185):
Gain root access and installs servers for SSH, SFTP, Samba, HTTP... You
can turn these services on/off from the settings page later.

As I said, this wasn\'t something I needed. I have only used the ssh
server a couple times to connect to the PocketBook without having to use
the USB.

### Conclusion

This brand was the most *neutral* I could find in terms of lock-ins.
Runs Linux, is easily hackable, can read most eBook formats and doesn\'t
force any cloud services.

# Posibles temas

## Server configuration

### Nginx configuration

During the past year, I\'ve done the following configurations:

-   Added virtual hosts: nextcloud, jellyfin, kepair.eu
-   Changed default configurations in php module (max memory per app)
-   Added security headers (global and for each virtualhost)
-   Folder permissions (www-data)
-   Apache tuning

;;

1.  Install Nginx

    Install steps: [nginx: Linux
    packages](http://nginx.org/en/linux_packages.html#Ubuntu)

2.  Add sites:

    1.  kepair.eu

        A simple static site served with the following code:

            server {

                    root /var/www/kepair.eu;
                    index index.html index.htm index.nginx-debian.html;

                    server_name kepair.eu;

                    location / {
                            try_files $uri $uri/ =404;
                    }

                listen [::]:443 ssl ipv6only=on; # managed by Certbot
                listen 443 ssl; # managed by Certbot
                ssl_certificate /etc/letsencrypt/live/kepair.eu/fullchain.pem; # managed by Certbot
                ssl_certificate_key /etc/letsencrypt/live/kepair.eu/privkey.pem; # managed by Certbot
                include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
                ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot

            }
            server {
                if ($host = kepair.eu) {
                    return 301 https://$host$request_uri;
                } # managed by Certbot


                    listen 80;
                    listen [::]:80;

                    server_name kepair.eu;
                return 404; # managed by Certbot


            }

    2.  Jellyfin

        Following [Nginx \| Documentation - Jellyfin
        Project](https://jellyfin.org/docs/general/networking/nginx.html)

    3.  Seafile

        Following [Seafile
        Server](https://download.seafile.com/published/seafile-manual/deploy/deploy_with_nginx.md)
        documentation

    4.  home.kepair.eu Client-side certificates

        <https://fardog.io/blog/2017/12/30/client-side-certificate-authentication-with-nginx/>
        CA: ca.key pass phrase: cocina

        user.key pass phrase: relay

        pkcs: Rcocina3

3.  Security headers

    [Be very careful with your add~header~ in Nginx! You might make your
    site
    insec...](https://www.peterbe.com/plog/be-very-careful-with-your-add_header-in-nginx)

    <https://serverfault.com/questions/870452/override-csp-header-for-specific-location>

4.  Global (in snippets/global-headers.conf)

    1.  HTTP Strict Transport Security (HSTS)

        `add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;`

    2.  Content Security Policy (CSP)

        `add_header Content-Security-Policy "default-src 'self';" always;`

5.  Site specific headers

    1.  kepair.eu

        1.  X-Content-Type-Options

            `add_header X-Content-Type-Options "nosniff";`

        2.  X-Frame-Options

            `add_header X-Frame-Options "SAMEORIGIN";`

        3.  Feature-Policy and Permission-Policy

            [Goodbye Feature Policy and hello Permissions
            Policy!](https://scotthelme.co.uk/goodbye-feature-policy-and-hello-permissions-policy/)

### Nextcloud

[Nextcloud via Docker with nginx reverse proxy - Dennis\'
Notes](https://dennisnotes.com/note/20180831-nextcloud-docker-nginx-reverse-proxy/)

### Setting up Wireguard VPN

<https://www.wireguard.com/quickstart/>
<https://linuxize.com/post/how-to-set-up-wireguard-vpn-on-ubuntu-18-04/>
mejor: <https://www.stavros.io/posts/how-to-configure-wireguard/>

### Pihole

<https://docs.pi-hole.net/guides/webserver/nginx/> It needs php fpm. I
installed version 8 I had to change fpm pool configuration from:
`/run/run/php-fpm.sock` to `127.0.0.1:9000` in
\"/etc/php/8.0/fpm/pool.d/www.conf\"

[amazon ec2 - nginx php-fpm: Permission denied while connecting to
upstream -
...](https://serverfault.com/questions/823723/nginx-php-fpm-permission-denied-while-connecting-to-upstream)

## Give me root access, it\'s my device now

## Remember to run these

Update commands for services I use

-   PiHole
-   Nextcloud docker
-   transmission-openvpn
-   ...

## Internet finances

How can Internet be funded?

## Industry as a Service?

## Mejoras en Goaccess

He mejorado el filtrado de Goaccess bastante:

-   Usando grep como un primer filtro: Filto llamadas a php, llamadas de
    Nextcloud News, wp-logins y analytics
-   Cambiando el referer policy de nginx en pedroir.nz.conf a no-referer

The JavaScript integration is a good option for most, but you can also
use a no-JavaScript image-based tracker, integrate it in your backend
middleware, or parse log files.

<https://www.goatcounter.com/code/api#backend-integration>
